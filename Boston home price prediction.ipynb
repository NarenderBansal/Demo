{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Boston home price prediction.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"kNL2ptmnprvN","colab_type":"code","outputId":"c4dab5f6-b146-4783-a49c-78f51c20354d","executionInfo":{"status":"error","timestamp":1560335346446,"user_tz":-330,"elapsed":1610,"user":{"displayName":"Narender Bansal","photoUrl":"","userId":"07056133008506250280"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["\n","\n","from xgboost.sklearn import XGBRegressor\n","# from tpot import TPOTRegressor\n","from sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor,GradientBoostingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.linear_model import ElasticNetCV,RidgeCV\n","from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n","import numpy as np\n","import pandas as pd\n","import scipy.stats as stats\n","import matplotlib.pyplot as plt\n","import sklearn\n","#import statsmodels.api as sm\n","from sklearn.datasets import load_boston\n","from sklearn.linear_model import LinearRegression,LogisticRegression\n","from sklearn import model_selection\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split \n","from sklearn.metrics import mean_squared_error,r2_score,accuracy_score\n","import pickle\n","\n","boston = load_boston(return_X_y=False)\n","bos = pd.DataFrame(boston.data)\n","\n","names = ['CRIM',  'ZN',  'INDUS' , 'CHAS' , 'NOX',  'RM'  ,'AGE' , 'DIS',  'RAD',  'TAX',  'PTRATIO',   'B',  'LSTAT']\n","test=['Price']\n","X= pd.DataFrame(boston.data,columns=names).values\n","y= pd.DataFrame(boston.target,columns =test).values\n","\n","df=pd.DataFrame(boston.data,columns=names)\n","df1=pd.DataFrame(boston.target,columns =test)\n","\n","\n","\n","\n","#print(df.head(5))\n","\n","#print(df.AGE.unique)\n","\n","# from sklearn.model_selection import train_test_split \n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.23, \n","                                                   random_state=4)\n","\n","regr = LinearRegression() \n","   \n","# Train the model using the training sets \n","regr.fit(X_train, y_train)\n","\n","\n","\n","# with open('LinearRegression.pickle', 'wb') as handle:\n","#   pickle.dump(regr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# with open('TpotRegression.pickle', 'rb') as handle:\n","# \ttpot = pickle.load(handle)\n","  \n","# with open('LinearRegression.pickle', 'rb') as handle:\n","# \tregr = pickle.load(handle)\n","\n","preds=regr.predict(X_test)\n","sub1 = pd.DataFrame(data=preds)  \n","\n","\n","print('Accuracy:', regr.score(X,y))\n","print('Accuracys:',accuracy_score(X.round(),y.round(),normalize=False))\n","\n","\n","print(\"Mean squared error: %.2f\" % mean_squared_error(y_test,preds))\n","\n","print(\"r2_score: %.2f\" % r2_score(y_test,preds))\n","# print(metrics.accuracy_score(y_test,preds))\n","print(metrics.explained_variance_score(y_test,preds))\n","\n","\n","rf = RandomForestRegressor(max_features=2, min_samples_split=4, n_estimators=50, min_samples_leaf=2)\n","gb = GradientBoostingRegressor(loss='quantile', learning_rate=0.0001, n_estimators=50, max_features='log2', min_samples_split=2, max_depth=1)\n","ada_tree_backing = DecisionTreeRegressor(max_features='sqrt', splitter='random', min_samples_split=4, max_depth=3)\n","ab = AdaBoostRegressor(ada_tree_backing, learning_rate=0.1, loss='square', n_estimators=2000)\n","\n","# search_grid={'n_estimators':[500,1000,2000],'learning_rate':[.001,0.01,.1],'random_state':[1]}\n","# search=GridSearchCV(estimator=ab,param_grid=search_grid,scoring='neg_mean_squared_error',n_jobs=1,cv=10)\n","# search.fit(X,y)\n","# search.best_params_ \n","\n","rf.fit(X_train, y_train)\n","preds=rf.predict(X_test)\n","gb.fit(X_train, y_train)\n","ab.fit(X_train, y_train)\n","\n","print('rf:  ',rf.score(X_test,y_test))\n","print('gb:  ',gb.score(X_test,y_test))\n","print('ab:  ',ab.score(X_test,y_test))\n","print(\"Mean squared error: %.2f\" % mean_squared_error(y_test,preds))\n","\n","# model=XGBRegressor()\n","# model.fit(X_train, y_train)\n","# print('XGB: ' ,model.score(X_test,y_test))\n","\n","# results = []\n","# kfold = model_selection.KFold(n_splits=2, random_state=4)\n","# cv_results = model_selection.cross_val_score(regr, X, y, cv=kfold, scoring='r2')\n","# results.append(cv_results)\n","# print(cv_results)\n","\n","# msg = \"%f, (%f)\" % (cv_results.mean(), cv_results.std())\n","# print(msg)\n","\n","\n","# # tpot = TPOTRegressor(generations=2, population_size=50, verbosity=2)\n","# # tpot.fit(X_train, y_train)\n","\n","\n","# # with open('TpotRegression.pickle', 'wb') as handle:\n","# #   pickle.dump(tpot.fitted_pipeline_, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","\n","# with open('TpotRegression.pickle', 'rb') as handle:\n","# \ttpot = pickle.load(handle)\n","\n","# tpot_pred = tpot.predict(X_test)  \n","\n","# print(tpot.score(X_test, y_test))\n","\n","# # tpot.export('tpot_boston_pipeline.py')\n","\n","# print(\"Mean squared error: %.2f\" % mean_squared_error(y_test,tpot_pred))\n","\n","# results = []\n","# kfold = model_selection.KFold(n_splits=2, random_state=4)\n","# cv_results = model_selection.cross_val_score(regr, X, y, cv=kfold, scoring='r2')\n","# results.append(cv_results)\n","# print(cv_results)\n","\n","# msg = \"%f, (%f)\" % (cv_results.mean(), cv_results.std())\n","# print(msg)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Accuracy: 0.7386930479691552\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-0a5b4e5564d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracys:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass-multioutput and multiclass targets"]}]}]}